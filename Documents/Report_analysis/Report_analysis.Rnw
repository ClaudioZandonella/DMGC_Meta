
%----    Basic packages    ----%
\documentclass{article}  
\usepackage[T1] {fontenc} 		  % Font encoding
\usepackage [utf8] {inputenc}		% Encoding for the document
\usepackage[a4paper,includeheadfoot,top=2.4cm, bottom=2cm, left=2.4cm, right=2.4cm]{geometry}  % margin settings
\usepackage[english]{babel}      


%----    Packages for using Kable Extra    ----%
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}


%----    Other packages    ----%
\usepackage{tikz}       
\usepackage{graphicx}   % for including graphics
\usepackage{amsmath}    % for math equation
\usepackage{bm}         % for bold math
\usepackage{csquotes}
\usepackage{enumitem}  % for bold enumerate
\setlist[enumerate]{font=\bfseries}
\setlist[enumerate]{itemsep=0ex} % reduce space between items
\setlist[itemize]{itemsep=0ex} % reduce space between items
\usepackage[labelfont=bf]{caption}  % caption
\usepackage[style=apa, backend=biber]{biblatex}  % bibliografia
\addbibresource{DMGC_Meta.bib}

\usepackage{hyperref}    % ref between elements


%----    LaTeX Settings    ----%

\newcommand{\textstreach}{\renewcommand{\baselinestretch}{1.5}}{} % default line stretch
\newcommand{\codestreach}{\renewcommand{\baselinestretch}{1}}{}   % code line streach
   
\textstreach

%----    Todo    -----%
\newcommand{\todo}[1]{(\textcolor{red}{Todo: #1})}
%---------------
% Added comand to resolve bug of pgf path when looking for raster images
\let\pgfimageWithoutPath\pgfimage 
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[#1]{figure/#2}}
%---------------


%%%%%%%%%%%%%%%%%%%%%%          Settings        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

<<knitr_settings, echo=FALSE, include=F, cache=FALSE>>=
# Set root directory
#knitr::opts_knit$set(root.dir = normalizePath(".."))

# Chunks settings
knitr::opts_chunk$set(echo = FALSE, 
                      # Plot settings
                      dev = "tikz", dev.args=list(pointsize=12),fig.align='center',
                     # fig.height=3, fig.width=5,
                      # Code output width
                      tidy=TRUE, tidy.opts = list(width.cutoff = 87),
                      # Cache options
                      cache = TRUE, autodep=TRUE)

# Chunk theme
thm=knit_theme$get("bclear")
knitr::knit_theme$set(thm)
knitr::opts_chunk$set(background = c(.985, .985, .985))

@

<<R_settings, echo=FALSE, include=F, cache=FALSE>>=

system (paste ("biber", sub ("\\.Rnw$", "", current_input())))

library("tidyverse")
library("knitr")
library("kableExtra")
library("metafor")
library("drake")
library("robumeta")

# Option KableExtra
options(knitr.kable.NA = '')

## ggplot settings
theme_set(theme_classic()+
          theme(text = element_text(size=12)))

knitr::read_chunk("../../R/Functions.R")
source("../../R/Functions.R") # defines functions
source("../../R/Auxiliary_functions.R") # Auxiliary functions

#----    load drake results    ----

# Data
loadd(data_raw)
loadd(data_effect)
loadd(data)

# Descriptive statistics
loadd(table_n_effects_studies)
loadd(table_freq_pub)
loadd(table_freq_grade)
loadd(table_freq_math_area)
loadd(table_freq_device)
loadd(table_freq_weeks)
loadd(table_freq_dependence)
loadd(table_freq_mot)

# Multilevel meta-analysis
loadd(fit_rma_mv)
loadd(plot_forest)

# Romubeta analysis
loadd(fit_robumeta)

# Meta-analysis data_aggregated
loadd(fit_rma)

# Sensitivity correlations
loadd(sens_summary)
loadd(table_sens_summary)
loadd(plot_sens_summary)

# Sensitivity leave-one-out
loadd(table_loo_summary)
loadd(sens_cook_summary)
loadd(plot_cook)

# Publication-bias
loadd(trim_fill_aggregated)
loadd(egger_regression_N)
loadd(egger_regression_vi_dppc2)
loadd(rank_test)

# Moderator-Analysis
loadd(mod_rma_mv_weeks)
loadd(mod_rma_mv_mot)

loadd(table_moderator_analysis)


#-----    Labels    ----

# Labels

label_grade = c("Primary school","Secondary school")
label_math_area = c("Algebra","Geometry","Number","Multiple areas")
label_dependece = c("Independent experimental and control groups", "Independent experimental groups same control", "Multiple outcomes on same participants","Single effect")
label_mot = c("Expectancy", "Value", "Not differentiated")

label_author_y_n_old =  data%>%
  arrange(N_study)%>%
  select(author_y)%>%
  unique()

label_author_y =  data%>%
  select(author_y)%>%
  unique()%>%
  mutate(author_y = recode(author_y,
    `Miller and Robertson (2011)` = "Miller \\& Robertson (2011)",
    `Main and O'Rourke (2011)` = "Main \\& O'Rourke (2011)",
    `Miller and Robertson (2010)` = "Miller \\& Robertson (2010)"),
    `Sun-Lin and Chiou (2010)` = "Sun-Lin \\& Chiou (2010)")

label_author_y_n_new =  data%>%
  arrange(N_study)%>%
  select(author_y)%>%
  unique()%>%
  mutate(author_y = recode(author_y,
    `Miller and Robertson (2011)` = "Miller \\& Robertson (2011)",
    `Main and O'Rourke (2011)` = "Main \\& O'Rourke (2011)",
    `Miller and Robertson (2010)` = "Miller \\& Robertson (2010)"),
    `Sun-Lin and Chiou (2010)` = "Sun-Lin \\& Chiou (2010)")
@
  
  
% Document title info

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%         Title           %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Title
\begin{titlepage}

\centering
	\vspace*{2cm}
	{\huge \bf The effects of educational video-games on students motivation to math: A meta-analysis
in K-12}\\
	\vspace{1cm}
	{\Large List authors\par}
  \vspace{1cm}
  {List affiliations\par}
	\vspace{2cm}
	{\huge\bfseries Analysis Report\par}
	\vspace{1cm}
	{\Large\textit{Edited by}}
	
	\vspace{1em}
	{\Large Claudio Zandonella Callegher*}
	\vfill
	*e-mail address\par
	\href{mailto:claudio.zandonella@gmail.com}{claudiozandonella@gmail.com}

	\vfill

% Bottom of the page
	{\large Last compiled\\ \today\par}
	
	\vspace{2cm}

\clearpage

\clearpage
\pagenumbering{Roman} 


\tableofcontents

\clearpage

\listoffigures
\listoftables

\end{titlepage}

%----------------------------------------------------------------------------------%
%--------------------------         Introduction         --------------------------%
%----------------------------------------------------------------------------------%

\pagenumbering{arabic}
\section{Introduction}

In this report the statistical analyses of the article \textit{"The effects of educational video-games on students motivation to math: A meta-analysis
in K-12"} are presented. The aim of the meta-analysis was to synthesized results of the studies concerning the impact of educational video-games on students’ motivation towards mathematics.

Here, we will focus only on the statistical analysis. For theoretical aspects, study selection, and results interpretation the reader can refer directly to the article.

%------------------------
\subsection{Report sections}

The analysis report is divided into different sections:
\begin{itemize}
    \item{\textbf{Section~\ref{sec:Statistical_Approach}:} the statistical approach and the plan of analysis are presented.}
    \item{\textbf{Section~\ref{sec:Data_Preparation}:} the dataset is presented with a brief description of each variable and prepared for the analysis.}
    \item{\textbf{Section~\ref{sec:Descriptive_Statistics}:} the descriptive statistics are presented.}
    \item{\textbf{Section~\ref{sec:Meta-Analysis_Results}:} the results of the meta-analysis are described.}
\end{itemize}

\clearpage

%----------------------------------------------------------------------------------%
%--------------------------    Statistical Approach      --------------------------%
%----------------------------------------------------------------------------------%

\section{Statistical Approach}
\label{sec:Statistical_Approach}

In this section, the statistical approach and the plan of the meta-analysis are presented. Fist, we describe the measure of effect size used to evaluate effectiveness of the interventions. Subsequently, we discuss the reasons why multilevel meta-analysis is used to account for the dependence of multiple effect sizes within the same studies. Finally, the plan of the meta-analysis is summarized.

%------------------------
\subsection{Measure of effect size}

%-------------
\subsubsection{The pre- post- control group design}

Selected studies were characterized by a pre- post- control group design (PPC). In a PPC design, participants are randomly assigned to one (or more) experimental group and to one (or more) control group. Both groups are evaluated before (pre-test score) and after (post-test score) the experimental group is exposed to a treatment.

The PPC design allows to evaluate the efficacy of the treatment taking into account pre-existing differences between the two groups and  concurrent factors or events other than the treatment that produce changes in the outcome variable.

The efficacy of the treatment can evaluated considering the standardized mean change in both groups. The standardized mean change of each group is defined as the mean difference between post-test ($\mu_{post}$) and pre-test ($\mu_{pre}$) scores, divided by the standard deviation ($\sigma$):
\begin{equation} \label{eq:mean_change}
\delta= \frac{\mu_{post} - \mu_{pre}}{\sigma}.
\end{equation}

Thus, assuming common standard deviation ($\sigma$), the efficacy of the treatment can be defined  as the difference in standardized mean change between the experimental group and the control group:
\begin{equation} \label{eq:effect}
\Delta=\delta_{Eg} - \delta_{Cg} = \frac{ (\mu_{Eg,post} - \mu_{Eg,pre}) - (\mu_{Cg,post} - \mu_{Cg,pre})}{\sigma},
\end{equation}
where \textit{Eg} stands for experimental group and \textit{Cg} stands for control group.


%-------------
\subsubsection{The effect size estimate}

To estimate the effect size, there are three alternatives that differ in the way the common standard deviation ($\sigma$) is estimated. \textcite{morrisEstimatingEffectSizes2008} discuss and evaluate the three alternatives:
\begin{enumerate}
  \item{$\bm{d_{ppc1}}$}: the effect size is computed using separate estimates of pre-test standard deviation for the experimental group (\textit{Eg}) and the control group (\textit{Cg}). That is,
  \begin{equation}
  d_{ppc1}= c_{Eg} \frac{(M_{Eg,post}-M_{Eg,pre})}{SD_{Eg,pre}} - c_{Cg} \frac{(M_{Cg,post}-M_{Cg,pre})}{SD_{Cg,pre}},
  \end{equation}
  where $c_{Eg}$ and $c_{Cg}$ are bias adjustments for small samples, $M_{pre}$ and $M_{post}$ are the mean scores in the pre-test and post-test, and $SD_{pre}$ is the standard deviation in the pre test.
  \item{$\bm{d_{ppc2}}$}: the effect size is computed by pooling the data from the experimental and control group in the pre-test to estimate the population standard deviation. That is,
  \begin{equation}
  d_{ppc2}= c_{P} \left[ \frac{(M_{Eg,post}-M_{Eg,pre})- (M_{Cg,post}-M_{Cg,pre})}{SD_{pre}}\right],
  \end{equation}
  where $SD_{pre}$\footnote{$SD_{pre} = \sqrt{\frac{(n_{Eg}-1)SD_{Eg,pre}^2 + (n_{Cg}-1)SD_{Cg,pre}^2}{n_{Eg}+n_{Cg}-2}}$} is the pooled standard deviation in the pre-test scores, and $c_P$\footnote{$c_P = 1-\frac{3}{4(n_{Eg}+n_{Cg}-2)-1}$} is a bias adjustment for small sample size.
  \item{$\bm{d_{ppc3}}$}: the effect size is computed by pooling the data from the experimental and control group in both the pre-test and post-test to estimate the population standard deviation.That is,
  \begin{equation}
  d_{ppc3}= c_{PP} \left[ \frac{(M_{Eg,post}-M_{Eg,pre})- (M_{Cg,post}-M_{Cg,pre})}{SD_{pre+post}}\right],
  \end{equation}
  where $SD_{pre+post}$\footnote{$SD_{pre+post} = \sqrt{\frac{(n_{Eg}-1)SD_{Eg,pre}^2 + (n_{Cg}-1)SD_{Cg,pre}^2 + (n_{Eg}-1)SD_{Eg,post}^2 + (n_{Cg}-1)SD_{Cg,post}^2}{2(n_{Eg}+n_{Cg}-2)}}$} is the pooled standard deviation in the pre-test and post-test scores, and $c_{PP}$\footnote{$c_{PP} = 1-\frac{3}{4(2n_{Eg}+2n_{Cg}-4)-1}$} is a bias adjustment for small sample size.
\end{enumerate}
\textcite{morrisEstimatingEffectSizes2008} suggested the $d_{ppc2}$ as the favourite effect size estimate in terms of bias, precision, and robustness to heterogeneity of variance. Given the assumption of homogeneity of variance in the two populations, $d_{ppc2}$ allows a better estimate of the population standard deviation by pooling the data from the experimental and control groups in the pre-test. On the contrary, $d_{ppc1}$ uses separate estimates of the sample standard deviation for experimental and control groups and $d_{ppc3}$ includes in the pooled standard deviation also results from the post-test of the two groups. These solutions are not optimal as, in the first case, we do not take advantage of the assumption of homogeneity of variance, and in the second case post-test variances tend to be larger than pre-test variances given possible interaction between treatment effect and individual differences.

The formula to compute the variance of $d_{ppc2}$ is provided by \textcite[see p.373 eq.25]{morrisEstimatingEffectSizes2008}:
\begin{equation}\label{eq:dppc2_variance}
\sigma^2(d_{ppc2}) = 2 (c_p^2) (1-\rho) \left(\frac{n_{Eg}+n_{Cg}}{n_{Eg}n_{Cg}}\right) \left(\frac{n_{Eg}+n_{Cg}-2}{n_{Eg}+n_{Cg}-4}\right) \left(1+\frac{d_{ppc2}^2}{2(1-\rho) \left(\frac{n_{Eg}+n_{Cg}}{n_{Eg}n_{Cg}}\right)}\right)-d_{ppc2}^2,
\end{equation}
where $\rho$ is the the correlation between pre- and post-test scores, and $n_{Eg}$ and $n_{Cg}$ are the two groups sample size.

Following recommendations of the author, we used the $d_{ppc2}$ to estimate the effect size of the studies included in the meta-analysis.  


%------------------------
\subsection{Multilevel meta-analysis}

Traditional meta-analysis approaches are based on the assumption that each effect sizes is independent, so each study should contribute with only one effect size. However, dependency among effect sizes is very common. Studies could evaluate the same subjects on multiple outcomes providing measures that are clearly not independent, or, even when outcomes are measured on independent samples, multiple effects within the same study are not independent. In fact, they share other common aspect as the design of the experiment, instruments used, treatment characteristics, research group, geographical region, etc., that may influence the effects to be more similar to each other.

In order to take into account the dependency between effect sizes, different approaches are proposed \parencite{pigottMethodologicalGuidancePapers2019,moeyaertMethodsDealingMultiple2017}:
\begin{enumerate}
    \item{\bf Averaging effect sizes}. When similar measures of the same underlying construct are provided, it is reasonable to use the average of the effect sizes to summarize the study results. \textcite[see Part 5: Complex data structure]{borensteinIntroductionMetaanalysis2009} describe how to compute a composite effect size and variance taking according to the dependence structures of the effect size within the study.
    \item{\bf Robust variance estimation (RVE)}. \textcite{hedgesRobustVarianceEstimation2010} recommend to use the RVE  when evaluating dependent effects in a meta-analysis. The overall effect size over studies is computed as a weighted mean of the observed effect size and the sampling variance estimate is obtained by means of an approximation of the covariance-matrix of each study. \textcite{hedgesRobustVarianceEstimation2010} demonstrate that RVE meta-analysis with non-independent effect sizes produces valid results even in the case of  misspecified covariance structure. As the number of the studies increase the results will converge to the correct values. Moreover, \textcite{tiptonSmallSampleAdjustments2015} provided adjusted estimators to the RVE that increase the reliability of the results even when the number of studies is small.
    \item{\bf Multilevel meta-analysis}. All random-effects meta-analysis are multilevel as they assume a two-level structure where the observed effects are an estimation of each study "true" effect that is sampled from an overarching distribution of "true" effects. However, in this case we refer to three-level meta-analysis model, where participants are included at level 1, measured outcomes at level 2, and study at level 3. In this three level approach, the dependency between effect size within studies is automatically accounted for in the covariance matrix \parencite{vandennoortgateMetaanalysisMultipleOutcomes2015}.
\end{enumerate}

\textcite{moeyaertMethodsDealingMultiple2017} compared the three approaches and reported that  both the RVA and the multilevel approach give unbiased parameter estimates, standard errors and variances. Whereas, averaging effect sizes is too conservative because in general standard errors are overestimated. Moreover, the RVA and the multilevel approach allow to include in the  meta-analytic model predictors at the outcome level to account for differences between outcomes within the same studies.

In the present meta-analysis, we decided to adopt a multilevel approach as it allows us to specify the kind of dependency between effect size specifically for each study. Multilevel meta-analysis offers a great flexibility in accommodating different type of dependency among effect sizes within studies.

On the contrary, RVA approach allows only to decide between a correlated effects dependence structure or a hierarchical dependence structure for all the studies \parencite{hedgesRobustVarianceEstimation2010, tiptonSmallSampleAdjustments2015}. In the first case, within each study outcomes are assumed to be measured on the same underlying units. In the second case, multiple (independent) outcomes are measured within studies, but they are related by other aspects such as research organizations, research labs, or research groups.

%------------------------
\subsection{Analysis plan}

The analysis are conducted with R software \parencite[version \Sexpr{paste0(c(version$major,".",version$minor,";"), collapse="")}][]{rcoreteamLanguageEnvironmentStatistical2018}. First, we load and prepare the dataset for the analysis, computing the effect size estimate and variance for each outcome. As presented in Formula~\ref{eq:dppc2_variance}, values of the correlation between pre-test and post-test scores are needed to the compute $d_{ppc2}$ variance. However, none of the studies reported this information. To overcome this issue, $d_{ppc2}$ variance is computed with different correlation values and their influence on the meta-analysis results will be subsequently evaluated.

Next, descriptive statistics are presented to evaluate the main characteristics of the data included in the analysis.

Multilevel meta analysis is conduct using the restricted maximum likelihood method with the R-package \texttt{metafor} \parencite{viechtbauerConductingMetaanalysesMetafor2010}. Variance-covariance matrix is defined for each study according to the specific outcomes dependence structure. However, studies with multiple measure on the same subjects did not reported the correlation between outcomes needed to specify dependence structure. In these cases, variance-covariance matrix is computed with different correlation values and their influence on the meta-analysis results will be subsequently evaluated. Results are presented and heterogeneity between studies is assessed through inspection of forest plot and evaluation of the Q-statistic \parencite{hedgesStatisticalMethodsMetaanalysis2014}. Under the null hypothesis, the Q-statistic is distributed as a chi-square with degrees of freedom equal to the number of studies minus one. A significant chi-value indicates the presence of heterogeneity across studies. Moreover, to estimate the magnitude of the heterogeneity the $I^2$ index is reported \parencite[i.e., the proportion of observed variance that reflects real and not random difference between studies effect sizes;][]{borensteinIntroductionMetaanalysis2009}. High values of $I^2$ suggest that differences between results are related to real differences across studies (i.e., different constructs or different study design). On the contrary, low values of $I^2$ suggest that results across studies are similar and possible differences are related to random sampling.

Next, we present the alternative results obtained using the different meta-analytic approaches presented above, which are robust variance estimation (RVE) approach and averaging effect sizes approach. RVE is conducted using the R-package \texttt{robumeta} \parencite{fisherRobumetaRpackageRobust2015, fisherRobumetaRobustVariance2017} considering a correlated effects dependence structure. Averaged effect sizes are obtained according to \textcite{borensteinIntroductionMetaanalysis2009} indications as implemented in the R-package \texttt{MAd} \parencite{delreMAdMetaanalysisMean2014}.

To investigate robustness of the results, a sensitivity analysis is conducted to evaluate how the values of the correlation between pre-test and post-test scores and the values of the correlation between dependent outcomes influence the meta analysis results. Moreover, a \textit{Leave-One-Out} (LOO) sensitivity analysis considering the study grouping level is run to assess the possible presence of influential studies. Substantial changes when a single study is removed would be interpreted as lack of homogeneity and unreliable results \parencite{viechtbauerOutlierInfluenceDiagnostics2010}.

Subsequently, publication bias is assessed. To our knowledge, there are no specific methods designed to correctly evaluate publication bias in a multilevel meta-analysis. Funnel plot is presented as a graphical representation of the symmetry of the effects distribution. However, this is not optimal as studies with multiple effects are overrepresented in the plot respect to studies with only one effect. To overcome this issue, we conduct separately two Egger’s regression tests to evaluate if effect sizes are associated to sample size or sampling variance of the estimated effect. In the absence of publication bias, the regression coefficient is expected to be zero \parencite{linQuantifyingPublicationBias2018, eggerBiasMetaanalysisDetected1997}. Moreover, the result of the rank correlation test is reported to examine whether the observed outcomes and the corresponding sampling variances are correlated \parencite{beggOperatingCharacteristicsRank1994}.

Alternatively, funnel plot with the \textit{trim-and-fill} method \parencite{duvalTrimFillSimple2000,rothsteinPublicationBiasMetaanalysis2005} is presented considering the aggregated effects.

Finally, the role of possible moderators is examined using mixed-effects meta-regression models, the moderators are included as a fixed effects and are tested using Wald’s chi-square \parencite{viechtbauerConductingMetaanalysesMetafor2010}. Considering the reduced number of studies and the unequal distribution among the different levels of the moderators, separate analyses are conducted for each moderator. Results have to be interpreted with caution as moderators are not evaluated together to understand the unique variance explained by each moderator.

%------------------------
\subsection{Analysis reproducibility}

To guarantee the reproducibility of the results, the whole analysis is structured within an \textit{R-project} named \texttt{DMGC\_Meta.Rproj} that is possible to download from this repository \todo{add repository link}.

The R-package \texttt{drake} \parencite{landauDrakePackagePipeline2018} is used to manage the analysis workflow and to enhance the readability and transparency of the analysis. To know more about \texttt{drake} consider the official Git-hub page (\url{https://github.com/ropensci/drake}) or the user manual(\url{https://books.ropensci.org/drake/}). Summarizing, using \texttt{drake} the code of the analysis is organized into different scripts. The user defines the plan of the analysis where each step in the analysis is defined trough functions. Functions can be appropriately defined to obtain desired targets (i.e., R-output with results of interests) and they are declared in another script. Subsequently, \texttt{drake} manages the whole analysis recognizing the dependency structure of the different targets. When any change is made to the code \texttt{drake} evaluates the analysis and updates the results. Using functions to define each step of the analysis allows to avoid "\textit{coping and paste}" in the code, it makes debugging easier, and it facilitates the reading of the code.

Moreover, the R-package \texttt{renv} \parencite{usheyRenvProjectEnvironments2019} is used to manage the dependencies of the R-packages used in the analysis. The \texttt{renv} package allows to create an isolated, portable, and reproducible environment where the analyses are run. However, \texttt{renv} is limited as it can not handle different R versions. Thus, to be fully reproducible users should use R version \Sexpr{paste0(c(version$major,".",version$minor), collapse="")}. To know more about \texttt{renv} consider the official documentation (\url{https://rstudio.github.io/renv/articles/renv.html}).

Finally, git version control was used to track the changes during the analysis.

%-------------
\subsubsection{R-project structure}

The R-project \texttt{DMGC\_Meta.Rproj} is organized into different folders. In the folder \texttt{Data/}, the raw datasets with the information regarding the studies selected in the literature review are stored.

In the folder \texttt{R/}, the R-scripts used in the analysis are stored. Using the \texttt{drake} package the analysis is organized into different R-scripts files:
\begin{itemize}
  \item{\texttt{Settings.R}} contains the setting for the R sessions, including R-packages used. 
  \item{\texttt{Plan.R}} contains the plan of the analysis. Where each target (i.e., R-output with results of interests) is defined through functions.
  \item{\texttt{Function.R}} contains the main functions used in \texttt{Plan.R} to obtain the targets of interest.
  \item{\texttt{Auxiliary\_functions.R}} contains other functions used in the analysis.
  \item{\texttt{Analysis.R}} is the script used to run the whole analysis.
\end{itemize}

In the folder \texttt{Documents/}, it is possible to find the material used to compile the \textit{Analysis Report} (see folder \texttt{Report\_analysis}) and the \textit{Prisma flow diagram} (see folder \texttt{Prisma}).

%-------------
\subsubsection{Run the Analysis}

To run the analysis you can follow two options: \textbf{A)} recreate a more reproducible enviroment using the same R version and packages versions; \textbf{B)} install the required packages and run the analysis.

\paragraph{Option A: reproducible enviroment using \texttt{renv}.}

\begin{enumerate}
  \item{Install R version \Sexpr{paste0(c(version$major,".",version$minor), collapse="")} from CRAN and select it as the the R version used in RStudio (more info at \url{https://support.rstudio.com/hc/en-us/articles/200486138-Changing-R-versions-for-RStudio-desktop}).}
  \item{Make sure you have already the \texttt{renv} R-package installed in your library. If not, run the command in R or R-studio \texttt{install.packages("renv")}}
  \item{Open the R-project \texttt{DMGC\_Meta}  by double-clicking the file \texttt{DMGC\_Meta.Rproj} you can find in the main directory. A new R-studio session should open and a similar message should appear in the console if \texttt{renv} was correctly installed:\\
    \texttt{* Project '$\sim$/<your\_path>/DMGC\_Meta' loaded. [renv <version\_number>]}}
  \item{Run the line \texttt{renv::restore()}, \texttt{renv} will ask the permission to install the R-packages used in the analysis, type \texttt{y} and return to confirm. If this fails, it could be due to \texttt{renv} attempting to install packages from sources but system prerequisites for compilation of a package are missing. In this case run \texttt{renv::equip()} and try again (only supported on Windows; if using other operative systems ensure to have the required development tools). If this does not solve the problem, just run the analysis following option B described below.}
  \item{Open the file `R/Analysis.R` and run each line of the sections "Load", "Check", and "Make".}
  \item{Now you can access the targets with the results using the functions \texttt{drake::loadd(<name\_target>)} and \texttt{drake::readd(<name\_target>)}.}
\end{enumerate}

\paragraph{Option B: install required packages.}

\begin{enumerate}
  \item{Open the R-project \texttt{DMGC\_Meta}  by double-clicking the file \texttt{DMGC\_Meta.Rproj} you can find in the main directory. A new R-studio session should open.}
  \item{Install the required packages running the line \texttt{install.packages(c("conflicted",  "tidyverse", "metafor", "clubSandwich", "drake", "gridExtra", "MAd", "visNetwork", "robumeta"))}}
  \item{Open the file `R/Analysis.R` and run each line of the sections "Load", "Check", and "Make".}
  \item{Now you can access the targets with the results using the functions \texttt{drake::loadd(<name\_target>)} and \texttt{drake::readd(<name\_target>)}.}
\end{enumerate}

\clearpage

%----------------------------------------------------------------------------------%
%------------------       Data Presentation and Preparation     -------------------%
%----------------------------------------------------------------------------------%

\section{Data Preparation}
\label{sec:Data_Preparation}

In this section, first the raw dataset is presented, then the dataset is prepared for the analysis.
%------------------------
\subsection{Data presentation}

In the folder \texttt{Data/}, you can find the file \texttt{Dataset.csv} with all the informations about the selected studies. The dataset includes \Sexpr{nrow(data_raw)} effects sizes grouped within \Sexpr{length(unique(data_raw$study))} different studies. Each line of the dataset is an effect and the characteristics of each effect are summarized in \Sexpr{ncol(data_raw)} variables:
\begin{enumerate}
  \item{\texttt{study}} - Numerical variable that indicates the id number of the study. This id number is used in the analysis to refer to a specific study. Values range from \texttt{1} to \texttt{20}.
  \item{\texttt{id}} - Numerical variable that indicates the id number of the study used during the selection process of eligible studies.
  \item{\texttt{author}} - Character string with the names of the authors.
  \item{\texttt{year}} - Numerical variable that indicates the year of publication.\footnote{In the case of unpublished dissertations or grey-literature the year refers to the realization of the study}.
  \item{\texttt{pub}} - Character string that indicates if the study is published in peer-reviewed journal (\texttt{yes}) or conferece papers and unpublished dissertations (\texttt{no}).
  \item{\texttt{grade}} - Numerical variable that indicates school grade of the participants. Primary school is indicated with \texttt{1} and secondary school is indicated with \texttt{2}.
  \item{\texttt{weeks}} -  Numerical variable that indicates the duration of the treatment in weeks.
  \item{\texttt{sessions}} - Numerical variable that indicates the total number of sessions of the treatment.
  \item{\texttt{minutes}} - Numerical variable that indicates average time in minutes of each session of the treatment.
  \item{\texttt{math\_area}} - Character string that indicates the math area target of the treatment. Algebra (\texttt{algebra}), geometry (\texttt{geometry}), number (\texttt{number}), or multiple areas (\texttt{multiple\_areas}).
  \item{\texttt{device}} - Character string that indicates the device used during the treatment. Personal Computer (\texttt{pc}), console (\texttt{con}), or application on other device (\texttt{app}).
  \item{\texttt{n\_effect}} - Numerical variable that identifies within each study the different effects. In each study the first effect is identified with \texttt{1}, then the value increases if more effects are reported within the same study. 
  \item{\texttt{id\_effect}} - Numerical variable that identifies the unique effect. Values range from \texttt{1} to \texttt{44}.
  \item{\texttt{dependence}} - Character string that indicates the type of dependence among different effect sizes reported within the same study. Study with single effect size (\texttt{none}), multiple effects measured on the same participants (\texttt{multiple\_outcomes}), multiple comparison between different experimental groups and same control group (\texttt{multiple\_groups}), or independent effect sizes (\texttt{independent}). 
  \item{\texttt{N}} - Numerical variable that indicates the total number of participants included to compute the considered effect. This is not always equal to the total number of participants in the study, as multiple experimental or control groups cold be included in the study.
  \item{\texttt{n\_cg}} - Numerical variable that indicates the number of participants in the control group for the considered effect.
  \item{\texttt{n\_eg}} - Numerical variable that indicates the number of participants in the experimental group for the considered effect.
  \item{\texttt{mot}} - Numerical variable that indicates if the considered effect measured the motivation in terms of expectancy (\texttt{1}), in terms of value (\texttt{2}), or did not differentiate between the two aspects (\texttt{NA}).
  \item{\texttt{m\_t1\_cg}} - Numerical variable that indicates the control group mean score in the pre-test.
  \item{\texttt{sd\_t1\_cg}} - Numerical variable that indicates the control group standard deviation in the pre-test.
  \item{\texttt{m\_t2\_cg}} - Numerical variable that indicates the control group mean score in the post-test.
  \item{\texttt{sd\_t2\_cg}} - Numerical variable that indicates the control group standard deviation in the post-test.
  \item{\texttt{m\_t1\_eg}} - Numerical variable that indicates the experimental group mean score in the pre-test.
  \item{\texttt{sd\_t1\_eg}} - Numerical variable that indicates the experimental group standard deviation in the pre-test.
  \item{\texttt{m\_t2\_eg}} - Numerical variable that indicates the experimental group mean score in the post-test.
  \item{\texttt{sd\_t2\_eg}} - Numerical variable that indicates the experimental group standard deviation in the post-test.
\end{enumerate}

The dataset is loaded in R and its structure is presented below.
\codestreach
<<load_dataset,eval=FALSE, echo=TRUE>>=
@
<<str_data_raw, echo=TRUE>>=
str(data_raw)
@
\textstreach
%------------------------
\subsection{Data munging}
\label{sec:munging}

Following \texttt{drake} guidelines, we define function for each step of the analysis. To organize the dataset we define the following function that allows to state which variables are categorical variables (factors) and uses more explicit labels in some variables. Moreover, we create a variable \texttt{author\_y} to obtain a label in the format \textit{"Author's (year)"} for each study and the variable \texttt{intensity = sessions*minutes/weeks} as a rough measure of the weekly intensity of each treatment.

\codestreach
<<clean_data, echo=TRUE, eval=FALSE>>=
@

In \textcite{paretoTeachableAgentArithmeticGame2011} the gain scores (i.e., the mean difference between post-test and pre-test scores) are reported instead of the post-test scores. Thus, to obtain the post-test scores we define a function that computes the post-test scores. The mean is given by the sum between the mean pre-test score and the mean gain score ($Mean_{post}=Mean_{pre}+Mean_{gain}$). The post-test Standard deviation is given by the square-root of the sum between the variance of the pre-test scores and the variance of the gain scores ($SD_{post}=\sqrt{SD_{pre}^2+SD_{gain}^2}$).

<<pareto_post,echo=TRUE, eval=FALSE>>=
@

%------------------------
\subsection{Compute $d_{ppc2}$ value and variance}
\label{sec:effect_variance}

To compute the $d_{ppc2}$ value and variance, we use the functions reported in the \texttt{metafor} website (\url{http://www.metafor-project.org/doku.php/analyses:morris2008}). To compute the variance we need the correlation between pre-test and post-test scores. However, correlation are not reported so we try with different values high correlation (\texttt{r\_high = .8}), medium-high correlation (\texttt{r\_mediumh = .6}), medium-low correlation (\texttt{r\_mediuml = .4}), and low correlation (\texttt{r\_low = .2}).

<<compute_dppc2, echo=TRUE, eval=FALSE>>=
@

<<load_data_effect>>=
show_hung <- data_effect%>%
  filter(study=="13", r_size=="r_mediumh")%>%
  select("study":"year","n_effect":"sd_t2_eg","sd_pool":"vi_dppc2", -"mot")

@

Simply looking at the effect obtained, we observe a strange value. In \textcite{hungEffectsDigitalGamebased2014} values of \texttt{yi\_dppc2} for the second effect are extremely huge \Sexpr{show_hung%>%filter(id_effect==33)%>%select(yi_dppc2)%>%round(.,1)}. This value is impossible.

<<show_hung>>=
show_hung
@

Considering the measure in the pre-test we can understand the problem. The standard deviation in the pre-test are extremely low: \Sexpr{show_hung%>%filter(id_effect==33)%>%select(starts_with("sd_t1"))%>%round(.,2)%>%paste(., collapse=" and ")}. This leads to a low value for the pooled standard deviation and in turns an implausible high value of effect size. That is probably given by the fact that the measure used was not able to evaluate properly the variation among individuals in the pre-test (maybe floor effect).

We do not include this effect in the meta-analysis and we remove it from the dataset. Thus the final dataset includes \Sexpr{length(unique(data$id_effect))} different effects.

<<rm_hung, echo=TRUE, eval=FALSE>>=
@


\clearpage

%----------------------------------------------------------------------------------%
%--------------------------    Descriptive Statistics    --------------------------%
%----------------------------------------------------------------------------------%

\section{Descriptive Statistics}
\label{sec:Descriptive_Statistics}

In this section we describe the main characteristics of the studies included in the meta-analysis. First we present descriptive statistics considering the study level, then we present descriptive statistics considering the effect sizes level.

%------------------------
\subsection{Studies characteristics}

In the analysis we include \Sexpr{length(unique(data$study))} studies and \Sexpr{length(unique(data$id_effect))} different effects. Studies were published between \Sexpr{min(data$year)} and \Sexpr{max(data$year)} and most of the studies are published after 2010. The number of studies according to year of publication is presented in Figure~\ref{fig:Plot_dist_year}.

<<Plot_dist_year, fig.pos="H", fig.align="center", fig.height=3, fig.width=4, fig.cap="Number of studies according to year of pubblication ($n_{studies}=20$).", fig.scap="Number of studies according to year of pubblication">>=

readd(plot_publication_year)
@

The majority of the studies were published in a peer reviewed journal, \Sexpr{table_freq_pub$n[1]} studies instead were obtained from conference papers or unpublished dissertations (see Table~\ref{tab:Table_freq_pub}).

\codestreach
<<Table_freq_pub>>=

table_freq_pub%>%
  arrange(rev(n))%>%
  mutate(type=c("Peer-reviewed journal","\\makecell[r]{Conference papers or \\\\dissertations}"))%>%
  select("type", "n") %>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Publication","Frequency"),caption = "Type of publication.",escape=F) %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$",footnote_as_chunk = T,escape=F)
@

The total number of participants included in each study range from \Sexpr{min(data$N)} to \Sexpr{max(data$N)} and the majority of the studies included less than 150 participants. The number of participants for each study is presented in Figure~\ref{fig:Plot_dist_sample}.

<<Plot_dist_sample, fig.pos="H", fig.align="center", fig.height=5, fig.width=5, fig.cap="Number of participants included in each study ($n_{studies}=20$).", fig.scap="Number of participants included in each study">>=
readd(plot_participants_studies)
@


The studies included in the meta-analysis differ according to the school grade of the participants. In \Sexpr{table_freq_grade%>%filter(grade=="Primary")%>%select(n)} studies, participants came form primary schools and in the remaining studies participants came from secondary schools (see Table~\ref{tab:Table_freq_grade}).

<<Table_freq_grade>>=
table_freq_grade%>%
  mutate(grade=recode(grade, Primary = "Primary school", Secondary = "Secondary school"))%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("School grade","Frequency"),caption = "Participants school grade.",escape=F) %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$",footnote_as_chunk = T,escape=F)
@

Regarding the math area target of the treatment, \Sexpr{table_freq_math_area%>%filter(math_area=="number")%>%select(n)} studies focused on numbers, \Sexpr{table_freq_math_area%>%filter(math_area=="algebra")%>%select(n)} studies on algebra, only one study on geometry and \Sexpr{table_freq_math_area%>%filter(math_area=="multiple_areas")%>%select(n)} focused on multiple areas (see Table~\ref{tab:Table_freq_math_area}).

<<Table_freq_math_area>>=
table_freq_math_area%>%
  mutate(math_area = label_math_area)%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Math area","Frequency"),caption = "Math area target of the treatment.",escape=F) %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$",footnote_as_chunk = T,escape=F)
@


Considering the device used in the treatment, the large majority of the studies used the Personal Computer. Only \Sexpr{table_freq_device%>%filter(device=="Console")%>%select(n)} studies used different type of console and \Sexpr{table_freq_device%>%filter(device=="App")%>%select(n)} studies involved the use of Applications (see Table~\ref{tab:Table_freq_device}).

<<Table_freq_device>>=
table_freq_device%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Device","Frequency"),caption = "Device used in the treatment.",escape=F) %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$",footnote_as_chunk = T,escape=F)
@

The duration of the treatment range from \Sexpr{min(data$weeks)} week to \Sexpr{max(data$weeks)} weeks. In Table~\ref{tab:Table_freq_weeks} studies are grouped according to approximate duration of the treatment.

<<Table_freq_weeks>>=
table_freq_weeks%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Duration","Frequency"),caption = "Duration of treatment in weeks.",escape=F) %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "Maximum value is 28 weeks; $n_{studies}=20$",footnote_as_chunk = T,escape=F)
@

In Table~\ref{tab:Table_n_studies_effects} the frequency of studies according to the number of effects is reported. Th majority of the studies include only one effect, all the other studies include between 2 and 4 effects, except one study with 8 effects.


<<Table_n_studies_effects>>=
table_n_effects_studies%>%
  select("n_effects","n_studies")%>%
  kable("latex", booktabs=T,align = rep("c",2),col.names = c("Effects within study", "Number of studies"),caption = "Frequency of studies according to the number of effects.") %>%
    row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$; $n_{effects}=43$",footnote_as_chunk = T,escape=F)
@

Studies with multiple effect size differ according to the type of dependency between effects. In \Sexpr{table_freq_dependence%>%filter(dependence=="multiple_outcomes")%>%select(n)} studies outcomes where measured on the same participants. Only \Sexpr{table_freq_dependence%>%filter(dependence=="multiple_groups")%>%select(n)} study included multiple independent treatment groups compared to a single control group and another study included multiple independent treatment groups and independent control groups (see Table~\ref{tab:Table_freq_dependence}). 

<<Table_freq_dependence>>=
table_freq_dependence%>%
  mutate(dependence = label_dependece)%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Dependency", "Frequency"),caption = "Type of dependency among effect sizes within the same study.") %>%
  row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$",footnote_as_chunk = T,escape=F)
@

%------------------------
\subsection{Effect characteristics}

In Figure~\ref{fig:Plot_dist_effects} the estimated effect sizes of each study are presented together with confidence intervals. The studies are ordered according to the total number of participants included and multiple outcomes from the same study are grouped together. We can observe the high variability between effects and how with the increasing of the sample size effects tend to be smaller.

<<Plot_dist_effects, fig.pos="t!", fig.align="center", fig.height=5, fig.width=6, fig.cap="Estimated effect size according to number of participants ($n_{studies}=20$; $n_{effects}=43$).", fig.scap="Estimated effect size according to number of participants">>=
readd(plot_effects_participants)
@

Considering the different measures of motivation, \Sexpr{table_freq_mot%>%filter(motivation=="expectation")%>%select(n)} effect sizes considered motivation in terms of expectancy and \Sexpr{table_freq_mot%>%filter(motivation=="value")%>%select(n)} effect sizes considered motivation in terms of value. The remaining effect sizes did not differentiate between the two aspects (see Table~\ref{tab:Table_freq_mot}).

<<Table_freq_mot>>=
table_freq_mot%>%
  mutate(motivation = label_mot)%>%
  kable("latex", booktabs=T,align = c("r","c"),col.names = c("Motivation measure", "Frequency"),caption = "Effect sizes according to the type of motivation measure.") %>%
  row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{effects} = 43$",footnote_as_chunk = T,escape=F)
@

\clearpage

%----------------------------------------------------------------------------------%
%--------------------------    Meta-Analysis Results     --------------------------%
%----------------------------------------------------------------------------------%

\section{Meta-Analysis Results}
\label{sec:Meta-Analysis_Results}

In this section, first we present the results of the meta-analysis. Then sensitivity analyses are performed to evaluate consistency of the results and the possible presence of publication bias. Finally, moderators analysis is described.

%------------------------
\subsection{Main results meta-analysis}

%-------------
\subsubsection{Variance-covariance matrix}
In order to compute a multilevel meta-analysis we need first to define for each study the variance-covariance matrix according to the specific dependence structure between effect sizes.

As we have seen in Table~\ref{tab:Table_freq_dependence} only \Sexpr{table_freq_dependence%>%filter(dependence != "none")%>%select(n)%>%sum()} studies include multiple effect size. In the case of two outcomes (A and B) measured on the same subjects, the covariance between outcomes ($cov(A, B)$) can be computed knowing the variance of the two outcomes ($\sigma_A^2$ and  $\sigma_B^2$) and their correlation ($\rho_{AB}$):
\begin{equation}
  cov(A, B) = \rho_{AB} \times \sigma_A\sigma_B.
\end{equation}
The same formula applies for each couple of outcomes when multiple outcomes were measured. The variance-covariance matrix ($\Sigma$) can be computed as the product between the diagonal matrix that includes the outcomes standard deviation ($diag(S)$) and the correlation matrix ($Corr$):
\begin{equation}
   \Sigma =  diag(S)\times Corr\times diag(S).
\end{equation}

However, none of the studies reported the correlation between values. Thus, variance-covariance matrix is computed with a range of different correlation values between outcomes and influence on the results is evaluated.

To compute the variance-covariance matrix in all studies, we used the \texttt{impute\_covariance\_matrix()} function included in the R-package \texttt{clubSandwich} \parencite{pustejovskyClubSandwichClusterrobustSandwich2019}. Given an assumed correlation value, the vector with all variances, and the grouping variable (in this case \texttt{study} as effects are grouped within studies), this function returns a list with the variance-covariance matrix for each study.
\Sexpr{table_freq_dependence%>%filter(dependence=="multiple_outcomes")%>%select(n)} studies outcomes where measured on the same participants. In the case of studies with only one effect, the \texttt{impute\_covariance\_matrix()} function returns a 1x1 matrix with sampling variance value.

\textcite{keAlternativeGoalStructures2008} is the only study where multiple independent treatment groups were compared to a common control group. In this case, covariance has to be computed considering this type of dependency between effects. \textcite{gleserStochasticallyDependentEffect2009} provided the formula to correctly compute the variance and covariance values for Cohen's d effect size when multiple experimental groups are compared to a common control group\footnote{The implemented R-functions are documented on the \texttt{metafor} website (\url{http://www.metafor-project.org/doku.php/analyses:gleser2009})}. However, to our knowledge, no formula is available in this case for the $d\_{ppc2}$ proposed by \textcite{morrisEstimatingEffectSizes2008}. Thus, we treat this study in the same way as studies with multiple measures on the same subjects. We allow correlation to vary in order to evaluate its influence on the results.

Finally, \textcite{ke2006computer} is the only study where multiple independent treatment groups were compared to multiple independent control groups. In this case the correlation between outcomes is set to zero.

We specify a function that given a correlation value computes and adjust the variance-covariance matrix for all studies according to dependency between effects.

<<compute_vcv_matrix, echo=TRUE, eval=FALSE, tidy.opts=list(width.cutoff = 80)>>=
@

%-------------
\subsubsection{Multilevel meta-analysis}

Based on the \texttt{rma.mv()} function from \texttt{metafor} R-package, we define a function to fit the multilevel meta-analysis. First we filter the dataset considering a give pre- post-test correlation (\texttt{r\_pre\_post} can be set to \texttt{"r\_high"},\texttt{"r\_mediumh"}, \texttt{"r\_mediuml"}, \texttt{"r\_low"}) used to compute the effect sampling variance (\texttt{vi\_dppc2}, see Section~\ref{sec:effect_variance}). The variance-covariance matrix is computed according to a given correlation value between outcomes (\texttt{r\_outcomes}). Then, the Multilevel meta-analysis is fitted considering the study clusters as a random-effects (\texttt{random =  $\sim$ 1|study}).

In the outcome we add the $I^2$ value computed following the functions presented on the \texttt{metafor} website (\url{http://www.metafor-project.org/doku.php/tips:i2_multilevel_multivariate}; see Section \textit{"Multilevel Models"}) and the coefficient test using the \texttt{coef\_test} from \texttt{clubSandwich} R-package that applies small sample correction. The function includes also two extra arguments that allow to exclude single studies and define moderators. These settings will be used for LOO sensitivity analysis and moderator analysis.

<<rma_multilevel, echo=TRUE, eval=FALSE, tidy.opts=list(width.cutoff = 73)>>=
@

Default settings consider \texttt{r\_pre\_post = "r\_mediumh"}, which means a correlation of 0.6, and \texttt{r\_outcomes = 0.5}. These are reasonable values researchers would expect. The influence of different values on the results is considered later. Below we report the output of the model \texttt{fit\_rma\_mv} based on default settings.

<<fit_rma_mv_summary,echo=TRUE >>=
summary(fit_rma_mv)
@

The model $I^2$ is \Sexpr{round(fit_rma_mv$I_squared,1)}\% and the coefficients test with small sample correction are reported in Table~\ref{tab:Table_coeff_fit_rma}
<<Table_coeff_fit_rma>>=
cbind(Coef="Intercept",
      fit_rma_mv$coef_test,
      sig="**")%>%
  mutate_at(2:5, round,2)%>%
  mutate_at(6, round, 4)%>%
  mutate_at(6, format, scientific=F)%>%
  kable("latex", booktabs=T,align = rep("c",7),col.names = c("Coefficient","Estimate","SE", "t-value","df","p-value", "Sig."),caption = "Coefficients test for \\texttt{fit\\_rma\\_mv}", row.names = FALSE) %>%
  row_spec(0,bold = T, align = "c") %>%
  kable_styling(position = "center", latex_options = "HOLD_position")%>%
  footnote(general = "$n_{studies}=20$; $n_{effects}=43$",footnote_as_chunk = T,escape=F)
@

The results indicate that the treatments have a significant small-effect ($d_{ppc2} = \Sexpr{round(fit_rma_mv$coef_test[,1],2)}$). However, the confidence interval ($95\% CI = [\Sexpr{paste0(round(fit_rma_mv$ci.lb,2),"; ", round(fit_rma_mv$ci.ub,2))}]$) range from very small-effects to medium effects indicating that there is a high level of uncertainty in the estimation. Moreover, results show high levels of heterogeneity in the studies included in the meta-analysis. The Q-statistic is significant $Q(\text{df}=\Sexpr{fit_rma_mv$k-1}) = \Sexpr{round(fit_rma_mv$QE,2)}$ and $I^2$ value is high ($I^2 = \Sexpr{round(fit_rma_mv$I_squared,1)}$\%). The Forest plot is presented in Figure~\ref{fig:Plot_forest_fit_rma}.

<<Plot_forest_fit_rma, fig.pos="!ht", fig.align="center", fig.height=8.5, fig.width=6.5, fig.cap="Forest plot of the multilevel meta-analysis ($n_{studies}=20$; $n_{effects}=43$).", fig.scap="Forest plot of the multilevel meta-analysis.">>=
plot_forest
@

%-------------
\subsubsection{Robust variance analysis}

In order to evaluate the result we would obtain using the Robust Variance Analysis (RVA) approach, we fit a different model using the function \texttt{robu()} from the \texttt{robumeta} R-package. We consider  a correlated effects dependence structure for the analysis (\texttt{modelweights = "CORR"}) and the same correlation between outcomes as in the previous analysis (\texttt{rho=.5})

<<rva_meta, echo=TRUE, eval=FALSE,tidy.opts=list(width.cutoff = 73)>>=
@

Results are presented below. Overall we have similar results to the previous analysis. The treatments effect is small and the heterogeneity between studies is high.

<<rva_meta_summary>>=
print(fit_robumeta)
@

%-------------
\subsubsection{Aggregated effects meta-analysis}
\label{sec:aggregated_meta}

In order to evaluate the result we would obtain averaging effects within studies, we conduct a separate meta-analysis. First, we compute the composite effect for each study using \texttt{agg()} function from \texttt{MAd} R-package \parencite{delreMAdMetaanalysisMean2014}. Following, \textcite{hoytEffectSizeCalculation2018} recommendations, we set \texttt{method="BHHR"} to use the approach suggested by \textcite{borensteinIntroductionMetaanalysis2009} and we set correlation between outcomes at 0.5 as in the previous cases.
<<aggregated_data, eval=F, echo=T, tidy.opts=list(width.cutoff = 73)>>=
@

Then, we fit a random-effects model meta-analysis using \texttt{rma()} function from \texttt{metafor} R-package. Results are presented below. Overall we have similar results to the previous analysis. The treatments effect is small and the heterogeneity between studies is high.

<<agg_meta_fit>>=
summary(fit_rma)
@

%------------------------
\subsection{Sensitivity analyses}

%------------------------
\subsubsection{Sensitivity correlation values}

To evaluate the influence of the correlation between pre-test and post-test scores (\texttt{r\_pre\_post}) and the influence of the correlation between outcomes (\texttt{r\_outcomes}), we run the multilevel meta-analysis trying different combination of values. The $d_{ppc2}$ sampling variance was previously computed considering high values (\texttt{r\_pre\_post} = 0.8), medium-high values (\texttt{r\_pre\_post} = 0.6), medium.low values (\texttt{r\_pre\_post} = 0.4), and low values (\texttt{r\_pre\_post} = 0.2) for the pre- post-test correlation (see Section~\ref{sec:effect_variance}).

For all conditions, several multilevel meta-analyses are run varying the correlation between outcomes used to compute the variance-covariance matrix. The sequence of values considered for the \texttt{r\_outcomes} range from 0.1 to 0.9 with increments of 0.1. Thus, in the sensitivity analysis \Sexpr{nrow(sens_summary)} different models are fitted.

The results of each different combination are saved in the R-object \texttt{sens\_summary} that can be loaded using the \texttt{drake} function \texttt{loadd(sens\_summary)}. The summary results are reported in Table~\ref{tab:Table_sens_summary} and presented in Figure~\ref{fig:Plot_sens_summary} (in the plot 95\% CI are not represented as they are mostly overlapping and this would make the plot messy and difficult to read).

<<Table_sens_summary>>=
table_report_sens = table_sens_summary%>%
  filter(names!="sigma2",names!="SE",names!="QE")%>%
  mutate(names=factor(names, levels=c("sigma","sigma_lb","sigma_ub","beta","beta_lb","beta_ub","I_squared")))%>%
  arrange(names)%>%
  mutate(names=c("Estimate","95\\% CI lb","95\\% CI ub","Estimate","95\\% CI lb","95\\% CI ub","$I^2$"))%>%
  mutate_at(vars(min:max), round,2)

kable(table_report_sens,"latex", booktabs=T,align = c("r",rep("r",3)),col.names = c("Parameter", "Min","Mean","Max"),caption = "Summary report of the correlations sensitivity analysis.", escape = FALSE) %>%
  row_spec(0,bold = T, align = "c") %>%
  kableExtra::group_rows(1,3,group_label = "$\\\\bf{\\\\sigma}$",escape = FALSE)%>%
  kableExtra::group_rows(4,6,group_label = "$\\\\bf{d_{ppc2}}$",bold=T,escape = FALSE)%>%
  kableExtra::group_rows(7,7,group_label = "Heterogeneity",escape = FALSE)%>%
  kable_styling(position = "center", latex_options = "HOLD_position")
  #footnote(general = "",footnote_as_chunk = T,escape=F)
@

Overall, results of the meta-analysis are only slightly influenced by the choice of the \texttt{r\_pre\_post} and \texttt{r\_outcomes} values. The standard deviation of the random effects of studies ($\sigma$) range from \Sexpr{table_report_sens[1,2]} to \Sexpr{table_report_sens[1,4]} with a mean value of \Sexpr{table_report_sens[1,3]}. The global effect of the treatments ($d_{ppc2}$) ranges from \Sexpr{table_report_sens[4,2]} to \Sexpr{table_report_sens[4,4]} with a mean value of \Sexpr{table_report_sens[4,3]}.

Grater values of \texttt{r\_pre\_post} (and therefore smaller values of effect sampling variance \texttt{vi\_dppc2}, see Section~\ref{sec:effect_variance}) lead to greater values of $\sigma$, $d_{ppc2}$, and $I^2$. On the contrary, greater values of \texttt{r\_outcomes} (and therefore greater values of variance in the estimations as the independent information in data is lower) lead to smaller values of $\sigma$, $d_{ppc2}$, and $I^2$.


<<Plot_sens_summary, fig.pos="!ht", fig.align="center", fig.height=8.6, fig.width=6.5, fig.cap="Results of the sensitivity analysis regarding \\texttt{r\\_pre\\_post} and \\texttt{r\\_outcomes}.", fig.scap="Results of the correlations sensitivity analysis.">>=
sens_summary_plot(sens_summary)
@

%------------------------
\subsubsection{\textit{Leave-One-Out} analysis}

To assess the possible presence of influential studies, we re-run the multilevel meta-analysis removing one study at time, thus we fit 20 different models. The complete results of each different model are saved in the R-object \texttt{sens\_loo\_summary} that can be loaded using the \texttt{drake} function \texttt{loadd(sens\_loo\_summary)}. The summary results are reported in Table~\ref{tab:Table_loo_summary} and presented in Figure~\ref{fig:Plot_loo_summary}.

<<Table_loo_summary>>=
table_report_loo = table_loo_summary%>%
  filter(names!="sigma2",names!="SE",names!="QE")%>%
  mutate(names=factor(names, levels=c("sigma","sigma_lb","sigma_ub","beta","beta_lb","beta_ub","I_squared")))%>%
  arrange(names)%>%
  mutate(names=c("Estimate","95\\% CI lb","95\\% CI ub","Estimate","95\\% CI lb","95\\% CI ub","$I^2$"))%>%
  mutate_at(vars(min:max), round,2)

kable(table_report_loo,"latex", booktabs=T,align = c("r",rep("r",3)),col.names = c("Parameter", "Min","Mean","Max"),caption = "Summary report of the LOO sensitivity analysis.", escape = FALSE) %>%
  row_spec(0,bold = T, align = "c") %>%
  kableExtra::group_rows(1,3,group_label = "$\\\\bf{\\\\sigma}$",escape = FALSE)%>%
  kableExtra::group_rows(4,6,group_label = "$\\\\bf{d_{ppc2}}$",bold=T,escape = FALSE)%>%
  kableExtra::group_rows(7,7,group_label = "Heterogeneity",escape = FALSE)%>%
  kable_styling(position = "center", latex_options = "HOLD_position")
  #footnote(general = "",footnote_as_chunk = T,escape=F)
@

Overall, results of the meta-analysis do not change considerably when one study is removed. The standard deviation of the random effects of studies ($\sigma$) range from \Sexpr{table_report_loo[1,2]} to \Sexpr{table_report_loo[1,4]} with a mean value of \Sexpr{table_report_loo[1,3]}. The global effect of the treatments ($d_{ppc2}$) ranges from \Sexpr{table_report_loo[4,2]} to \Sexpr{table_report_loo[4,4]} with a mean value of \Sexpr{table_report_loo[4,3]}.

<<Plot_loo_summary, fig.pos="!ht", fig.align="center", fig.height=4.7, fig.width=6.7, fig.cap="Results of the \\textit{leave-one-out} sensitivity analysis.", fig.scap="Results of the \\textit{leave-one-out} sensitivity analysis.">>=
readd(plot_sens_loo)
@

Moreover, to evaluate the presence of influential studies we compute Cook's distance for each study \parencite{cookDetectionInfluentialObservation1977}. Cook's distances are presented in Figure~\ref{fig:Plot_cook}. \textcite{rodriguez-aflechtNumberNavigationGame2015} has the highest Cook's distance ($D=\Sexpr{round(max(sens_cook_summary$cooks_distance),3)}$). To evaluate if a case is influential, several cut-offs have been proposed for Cook's distances. Few of these cut-offs are for example: $D_i> 1$; $D_i> 4/n$ where $n$ is the number of cases; $D_i> 4/(n-k-1)$ where $k$ is the number of model parameters. 

However, as suggested by \textcite{foxRegressionDiagnosticsVol1991}, a better option is to consider the graphical representation of Cook's distances to examine if there are cases that present a substantially larger distance than the rest. In Figure~\ref{fig:Plot_cook} we can observe a homogeneous distribution of distances. Thus, we can conclude that there are no influential cases. 

<<Plot_cook, fig.pos="!ht", fig.align="center", fig.height=3.5, fig.width=6.5, fig.cap="Cook's distance of each study ($n_{studies}=20$).", fig.scap="Cook's distance of each study.">>=
readd(plot_cook)
@

%------------------------
\subsection{Publication bias}

To evaluate the presence of publication bias in the studies selected in the meta-analysis, the Funnel plot in presented in Figure~\ref{fig:Plot_funnel_mv}. We can observe that the distribution of effects is not symmetric with respect to the estimated overall treatments effect ($d_{ppc2}$). Studies with larger sample size tend to report smaller effects, whereas studies with smaller sample size show greater effects.

However, this is not the optimal way to represent and evaluate the presence of publication bias, as studies with multiple effects are overrepresented in the plot respect to studies with only one effect.

<<Plot_funnel_mv, fig.pos="!ht", fig.align="center", fig.height=4.5, fig.width=5.5, fig.cap="Funnel plot of the multilevel meta-analysis ($n_{studies}=20$; $n_{effects}=43$).", fig.scap="Funnel plot of the multilevel meta-analysis.">>=
par(mar = c(4, 4, 0, 4))
funnel(fit_rma_mv)
@

To overcome this issue we conduct two Egger’s regression tests. The main idea is to evaluate if within studies the sample size (\texttt{N}) or the sampling variance (\texttt{vi\_dppc2}) are associated to the estimated effect size (\texttt{yi\_dppc2}). In absence of publication bias, studies results should be symmetrically distributed with respect to the estimated overall effect. Thus, studies results are expected to not be associated to sample size, sampling variance, or other indexes of the precision.

In the first Egger’s regression test, we add the sample size as moderator in the multilevel meta-analysis (\texttt{mod = $\sim$ N}). Results are presented below.

<<Egger_test_N>>=
summary(egger_regression_N)
@

In the second Egger’s regression test, we add the sampling variance as moderator in the multilevel meta-analysis (\texttt{mod = $\sim$ vi\_dppc2}). Results are presented below.

<<Egger_test_vi_dppc2>>=
summary(egger_regression_vi_dppc2)

egger_regression_N$coef_test[2,1]
@

In both cases the test of moderators is significant. For the sample size, we have $QM(df = \Sexpr{egger_regression_N$p.eff-1}) = \Sexpr{round(egger_regression_N$QM,2)}$, \textit{p-value}$ = \Sexpr{round(egger_regression_N$QMp,3)}$ and the parameter coefficient ($\beta = \Sexpr{paste(round(egger_regression_N$coef_test[2,1],5))}$) indicates that the expected effect size decreases as the study sample size increases. Note that even if the parameter value is small it refers to the effect of adding one participant, so for large sample size the changes may be remarkable.

For the sampling variance, we have $QM(df = \Sexpr{egger_regression_vi_dppc2$p.eff-1}) = \Sexpr{round(egger_regression_vi_dppc2$QM,2)}$, \textit{p-value}$ = \Sexpr{round(egger_regression_vi_dppc2$QMp,3)}$ and the parameter coefficient ($\beta = \Sexpr{round(egger_regression_vi_dppc2$coef_test[2,1],2)}$) indicates that, when the sapling variance decreases (i.e., more precise studies), the expected effect size decreases as well.

In addition to the previous tests, we also report results of the rank correlation test that confirm the correlation between the observed outcomes and the corresponding sampling variances \parencite{beggOperatingCharacteristicsRank1994}. 

<<rank_test, echo=T>>=
ranktest(fit_rma_mv)
@

Finally, we present the funnel plot with the \textit{trim-and-fill} method considering the results of the meta-analysis using aggregated effect sizes (see Section~\ref{sec:aggregated_meta}). The results are reported below and the and the funnel plot is represented in Figure~\ref{fig:Plot_funnel_aggregated}.

<<trim_fill>>=
trim_fill_aggregated
@

The results indicate that, to obtain symmetry, five hypothetical studies have to be added. The resulting treatments effect is still significant, but the effect size is smaller than the initial one ($d_{ppc2}= \Sexpr{round(trim_fill_aggregated$beta,2)}$).

<<Plot_funnel_aggregated, fig.pos="!ht", fig.align="center", fig.height=4.5, fig.width=5.5, fig.cap="Funnel plot with \\textit{trim-and-fill} method for meta-analysis with aggregated effects (white dots are the added studies; $n_{studies}=20$; $n_{effects}=43$).", fig.scap="Funnel plot with \\textit{trim-and-fill} method for meta-analysis with aggregated effects.">>=
par(mar = c(4, 4, 0, 4))
funnel(trim_fill_aggregated)
@

Overall, the results indicate the presence of publication bias.

%------------------------
\subsection{Moderators analysis}


In this last part of the analysis, we evaluate he role of possible moderators such as type of publication (\texttt{mod = $\sim$ pub}), school grade of the participants (\texttt{mod = $\sim$ grade}), duration of the treatment in weeks (\texttt{mod = $\sim$ weeks}), intensity of the treatment (\texttt{mod = $\sim$ intensity}; see Section~\ref{sec:munging}), math area target of the treatment (\texttt{mod = $\sim$ math\_area}), device used in the treatment (\texttt{mod = $\sim$ device}), and type of motivation measured (\texttt{mod = $\sim$ mot}).

All the moderators are variables at the studies level. Only \texttt{mot} is a variable at the outcomes level as within the same study measures can differ according to the type of motivation. Three-level meta-analysis allows to investigate also this aspect that would not be possible to evaluate otherwise.

For each moderator a separate model is fitted Thus, results have to be interpreted with caution as  moderators are not evaluated together to understand the unique variance explained by each moderator. Moreover, not all the studies include information about \texttt{intensity} and \texttt{mot}. In this cases the analysis is restricted to studies with complete data. Results of the moderator analysis are reported in Table~\ref{tab:Table_moderator_analysis}.

<<Table_moderator_analysis>>=

table_moderator_analysis%>%
  cbind(Sign=c(" "," ","*"," "," "," ","**"))%>%
  mutate(Moderator=ifelse(Moderator=="math_area", "math\\_area",as.character(Moderator)),
         Moderator=paste0("\\texttt{",Moderator,"}"),
         Q_value=round(Q_value,2),
         Q_pvalue=round(Q_pvalue,3))%>%
  
  kable("latex", booktabs=T,align = c("r",rep("c",5),"l"),col.names = c("Moderator", "Studies","Effects","Q-value","df","p-value","Sign."),caption = "Results of the moderators analysis", escape = FALSE,linesep = "") %>%
  row_spec(0,bold = T, align = "c")%>%
  kable_styling(position = "center", latex_options = "HOLD_position")
  #footnote(general = "",footnote_as_chunk = T,escape=F)
@

Looking at the test of moderators, we note that only the duration of the treatment in weeks and the type of motivation measured are significant moderators. To understand the direction of the effects we evaluate the model parameters.

Considering \texttt{weeks}, the effect of the treatment decreases as the number of weeks increases ($\beta = \Sexpr{round(mod_rma_mv_weeks$beta[2,],2)}$). Complete results are reported below.

<<mod_rma_mv_weeks>>=
mod_rma_mv_weeks
mod_rma_mv_weeks$beta
@

Considering \texttt{mot}, the effect of the treatment is greater if motivation is measured in terms of expectancy than if motivation is measured in terms of value ($\beta = \Sexpr{round(mod_rma_mv_mot$beta[2,],2)}$). Complete results are reported below.

<<mod_rma_mv_mot>>=
mod_rma_mv_mot
@

%------------------------
\subsection{Conclusions}

Overall, results indicate that there is a small effect of treatments. Moreover, treatments seems to have a larger effect on motivation in terms of \texttt{expectancy} than  motivation in terms of \texttt{value} and the duration of the treatments seems to be a moderator factor, with shorter treatments having larger effects than longer ones.

However, this results have to be considered with caution given the great heterogeneity between studies and the presence of  publication bias. 

\clearpage
%----------------------------------------------------------------------------------%
%--------------------------    Session Information   ------------------------------%
%----------------------------------------------------------------------------------%

\section{Session Information}

<<Session Info, echo=T>>=
  sessionInfo(package = NULL)
@
  
\clearpage

\section*{Appendix A}
\addcontentsline{toc}{section}{Appendix A: Studies included in the meta-analysis}
\begin{refsection}

\nocite{abdelhafezEffectsGamebasedTechnology2016}
\nocite{baiAssessingEffectiveness3D2012}
\nocite{changEffectsEducationalVideo2016}
\nocite{hungEffectsDigitalGamebased2014}
\nocite{ke2006computer}
\nocite{keAlternativeGoalStructures2008}
\nocite{kebritchiEffectsModernMathematics2010}
\nocite{kimGamebasedLearningOpenSimsupported2017}
\nocite{kuEffectsMinigamesStudents2014a}
\nocite{mainNewDirectionsTraditional2011}
\nocite{martinEffectGameBasedLearning2018a}
\nocite{mavridisImpactOnlineFlexible2017}
\nocite{mccueLearningMiddleSchool2011a}
\nocite{millerUsingGamesConsole2010}
\nocite{millerEducationalBenefitsUsing2011}
\nocite{paretoTeachableAgentArithmeticGame2011}
\nocite{riconscenteResultsControlledStudy2013}
\nocite{rodriguez-aflechtNumberNavigationGame2015}
\nocite{starkeyEffectsDigitalGames2013a}
\nocite{sun-linEffectsGamifiedComparison2019}

\printbibliography[heading=subbibliography, title= Studies included in the meta-analysis]
\end{refsection}
\clearpage

\phantomsection 
\addcontentsline{toc}{section}{Bibliography}
\printbibliography


\end{document}


\end{document}